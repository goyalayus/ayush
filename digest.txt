Directory structure:
└── ayush.site/
    ├── blog.html
    ├── index.html
    ├── peeple.html
    ├── style.css
    └── blog/
        ├── decoding-transformers.html
        ├── distribution-for-pmf.html
        ├── how-llm-tokenizers-work-bpe.html
        ├── imagegen-skill-issue.html
        ├── startup-context.html
        ├── students-who-stammer.html
        ├── summer-goals.html
        └── two-startups.html

================================================
File: blog.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Blog - Ayush</title>
        <meta name="description" content="A collection of blog posts by Ayush." />
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="style.css" />
        <style>
            .blog-post-item {
                margin-bottom: 2rem;
                padding-bottom: 1.5rem;
                border-bottom: 1px dashed var(--text-color);
            }
            .blog-post-item:last-child {
                border-bottom: none;
                margin-bottom: 0;
            }
            .blog-post-title {
                font-size: 1.3rem;
                font-weight: 700;
                margin-bottom: 0.5rem;
            }
            .blog-post-title a {
                text-decoration: none;
                color: var(--text-color);
            }
            .blog-post-title a:hover {
                color: var(--accent);
                text-decoration: underline;
            }
            .blog-post-excerpt {
                margin-bottom: 0.75rem;
            }
        </style>
    </head>

    <body>
        <header>
            <div class="profile">
                <a href="index.html">
                    <img
                        src="ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="index.html" class="nav-link">ABOUT</a>
                <a href="blog.html" class="nav-link active">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h1 class="section-title" style="font-size: 1.5rem; margin-bottom: 2rem;">Blog Posts</h1>
                
                <div class="blog-post-list">
                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/decoding-transformers.html">Decoding Transformers</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            An in-depth look at how Transformer models work, covering pretraining, batching, embeddings, attention mechanisms, and sampling strategies for LLMs.
                        </p>
                        <a href="blog/decoding-transformers.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/two-startups.html">2 Startups</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            Startups aren't a single entity but a two-part journey: pre-PMF and post-PMF. This post explains why the distinction is critical and how advice often misaligns.
                        </p>
                        <a href="blog/two-startups.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/distribution-for-pmf.html">Distribution for PMF</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            Founders with distribution have an advantage in finding PMF. This post explores how, the common pitfalls, and a potential solution for leveraging audience effectively.
                        </p>
                        <a href="blog/distribution-for-pmf.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/students-who-stammer.html">Students in Tech who stutter/stammer</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            An exclusive community for tech students who stutter/stammer. Learn why it's different, who's behind it, and how to join.
                        </p>
                        <a href="blog/students-who-stammer.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/how-llm-tokenizers-work-bpe.html">How LLM Tokenizers Work: Understanding Byte-Pair Encoding (BPE)</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            ChatGPT doesn't just 'understand' English; it processes numbers. This post explains how tokenizers, especially Byte-Pair Encoding (BPE), convert your text into a format LLMs can work with using a simplified example.
                        </p>
                        <a href="blog/how-llm-tokenizers-work-bpe.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/imagegen-skill-issue.html">Okay, so it is a skill issue</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            My journey from initial disappointment with GPT-4 image generation to understanding the crucial role of skilled prompting. Includes an example with a detailed prompt.
                        </p>
                        <a href="blog/imagegen-skill-issue.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/summer-goals.html">What am I doing this summer.</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            Posting your goals to world keeps you super accountable
                        </p>
                        <a href="blog/summer-goals.html" class="inline-link">Read more →</a>
                    </article>

                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/startup-context.html">Sharing all the context I've gathered</a>
                        </h2>
                        <p class="text blog-post-excerpt">
                            If you're looking to learn how to build a startup, start here. This post covers essential startup and technical context for aspiring builders.
                        </p>
                        <a href="blog/startup-context.html" class="inline-link">Read more →</a>
                    </article>

                    <!-- To add a new blog post, copy the structure from one of the posts above and update links/content -->
                    <!-- 
                    <article class="blog-post-item">
                        <h2 class="blog-post-title">
                            <a href="blog/another-post.html">Another Blog Post Title</a>
                        </h2>
                        <p class="text blog-post-excerpt">A brief summary of the other blog post...</p>
                        <a href="blog/another-post.html" class="inline-link">Read more →</a>
                    </article>
                    -->
                </div>
            </section>
        </main>
    </body>
</html>



================================================
File: index.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>ayush</title>
        <meta name="description" content="ayush's personal website" />
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="style.css" />
    </head>

    <body>
        <header>
            <div class="profile">
                <a href="index.html">
                    <img
                        src="ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="index.html" class="nav-link active">ABOUT</a>
                <a href="blog.html" class="nav-link">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h2 class="section-title">こんにちは！</h2>

                <p class="text">
                    I am <strong style="color: var(--accent);">ayush</strong>, pre-final at iit roorkee. currently I am
                    building
                    <a href="peeple.html" class="inline-link">peeple</a>. I am generally interested in
                    consumer startups, AGI, Party Dancing, skatebording and
                    comming up with insights like
                    <a href="https://x.com/goyalayus/status/1922024740639682799" class="inline-link"
                        >these</a
                    >.
                </p>

                <h3 class="section-title">
                    What important truth do very few people agree with me on?
                </h3>
                <p class="text">
                    soon AGI will make everyone financially free and immortal.
                </p>


                <h2 class="section-title">my works</h2>
                <ul class="list">
                    <li>
                        raised incubation for
                        <a href="peeple.html" class="inline-link">peeple</a> from
                        <a href="https://tides.iitr.ac.in/" class="inline-link">Tides</a> with my
                        buddies <a href="https://x.com/Sikandar_lx" class="inline-link" target="_blank">@Sikandar_lx</a> and <a href="https://x.com/arnnnvvv" class="inline-link" target="_blank">@arnnnvvv</a>
                    </li>
                    <li>Secretary of <a href="https://www.instagram.com/gdsc_iitr/" class="inline-link" target="_blank">gdsc</a> iitr</li>
                    <li>
                        built
                        <a href="https://ace-sand-pi.vercel.app/" class="inline-link">ace</a> that
                        cuts ml paper and code reading time by 8hrs/wk
                    </li>
                    <li>
                        built
                        <a href="https://github.com/arnnvv/cutcrap" class="inline-link"
                            >cutTheCrap</a
                        >, takes your book and gives you a much mroe efficient
                        100 page version of it.
                    </li>
                    <li>wrote websockets from scratch in c</li>
                    <li>wrote gpt-2 code from scratch</li>
                </ul>

                <h2 class="section-title">CONTACT</h2>
                <p class="text">
                    if you want to talk, vibe or build something cool with me
                    hit me up at
                    <a href="mailto:ayush_g@ar.iitr.ac.in" class="inline-link"
                        >ayush_g@ar.iitr.ac.in</a
                    >
                </p>

                <div class="social-links">
                    <a href="https://x.com/goyalayus" class="social-link"
                        >X/goyalayus</a
                    >
                    <a
                        href="https://github.com/goyalayus"
                        target="_blank"
                        class="social-link"
                        >GIT</a
                    >
                    <a
                        href="https://in.linkedin.com/in/ayush-goyal-45334418a"
                        target="_blank"
                        class="social-link"
                        >Linkedin</a
                    >
                </div>
            </section>
        </main>
    </body>
</html>



================================================
File: peeple.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Peeple</title>
        <meta name="description" content="Peeple - The world's most transparent dating app." />
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="style.css" />
        <style>
            body > header {
                padding-bottom: 0.5rem;
            }
            header .profile {
                margin-bottom: 0.7rem;
            }
            .tagline {
                font-size: 1.4rem;
                color: var(--text-color);
                margin-top: 0.1rem;
                text-align: left;
                font-weight: bold;
                padding-left: 0;
            }
            main section .section-title {
                color: var(--accent);
                border-bottom-color: var(--accent);
                display: inline-block;
            }
            header + main .section-title {
                margin-top: 3rem; 
            }
            .feature-list li {
                border-left: 4px solid var(--accent);
                padding-left: 1rem;
                margin-bottom: 2.5rem;
                list-style-type: none;
            }
            .feature-list strong {
                color: var(--text-color);
                font-weight: bold;
            }
            .feature-images {
                margin-top: 1rem;
                display: flex;
                gap: 1rem;
                flex-wrap: wrap;
                justify-content: flex-start;
            }
            .feature-images img {
                width: 150px;
                height: auto;
                border: 1px solid var(--text-color);
            }
        </style>
    </head>

    <body>
        <header>
            <div class="profile">
                <a href="peeple.html">
                    <img
                        src="peeple.jpeg"
                        alt="Peeple logo"
                        class="profile-img"
                    />
                    <h1 class="site-title">Peeple</h1>
                </a>
            </div>
            <p class="tagline">worlds most transparent dating app</p>
            <nav class="nav-links">
            </nav>
        </header>

        <main>
            <section>
                <h2 class="section-title">Here's how we're different, bringing a unique vibe:</h2>
                <ul class="list feature-list">
                    <li>
                        With our app, guys get <strong>free analytics</strong>.For example see how many women viewed your profile each week and how that compares to the average. Know your numbers.
                    </li>
                    <li>
                        You can literally <strong>query our database</strong> right inside the app. Ask questions like, "What's the average number of daily matches for the top 10% of male profiles?" and get answers straight from our data.
                    </li>
                    <li>
                        We sort profiles into two <strong>distinct 'vibe' buckets</strong>. Matching only happens within these curated groups, so profiles with completely different energies won't see each other, ensuring more aligned connections. For example, these two profiles would never see each other:
                        <div class="feature-images">
                            <img src="profile1.jpeg" alt="Profile example 1">
                            <img src="profile2.jpeg" alt="Profile example 2">
                        </div>
                    </li>
                    <li>
                        For women, all premium features in our app are <strong>completely free</strong>. Unlock the full experience, no strings attached.
                    </li>
                </ul>
            </section>
        </main>
    </body>
</html>



================================================
File: style.css
================================================
:root {
    --bg-color: #f6f6f6;
    --text-color: #191919;
    --accent: #ff5500;
}

* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: "Space Mono", monospace;
    background-color: var(--bg-color);
    color: var(--text-color);
    line-height: 1.5;
    padding: 2rem;
    max-width: 900px;
    margin: 0 auto;
    background-image:
        linear-gradient(rgba(0, 0, 0, 0.06) 1px, transparent 1px),
        linear-gradient(90deg, rgba(0, 0, 0, 0.06) 1px, transparent 1px);
    background-size: 20px 20px;
}

header {
    margin-bottom: 2rem;
    border-bottom: 2px solid var(--text-color);
    padding-bottom: 1rem;
}

.profile {
    display: flex;
    align-items: center;
    gap: 1.5rem;
    margin-bottom: 1.5rem;
}

.profile a {
    display: flex;
    align-items: center;
    gap: 1.5rem;
    text-decoration: none;
    color: var(--text-color);
}

.profile-img {
    width: 70px;
    height: 70px;
    object-fit: cover;
    border: 2px solid var(--text-color);
}

.site-title {
    font-weight: 700;
    letter-spacing: 2px;
    font-size: 1.8rem;
    text-transform: uppercase;
}

.nav-links {
    display: flex;
    gap: 1.5rem;
}

.nav-link {
    font-weight: 700;
    text-decoration: none;
    color: var(--text-color);
    text-transform: uppercase;
    font-size: 0.9rem;
}

.nav-link.active {
    color: var(--accent);
}

.section-title {
    display: inline-block;
    border-bottom: 2px solid var(--text-color);
    margin-top: 2rem; 
    margin-bottom: 1.5rem; 
    text-transform: uppercase;
    font-weight: 700;
    font-size: 1.2rem;
    letter-spacing: 1px;
}

main > section > .section-title:first-child {
    margin-top: 0;
}


.text {
    margin-bottom: 1.5rem;
}

strong {
    color: var(--accent);
}

.jp-divider {
    height: 2px;
    background-color: #ddd;
    margin: 2rem 0;
}

.list {
    list-style: none;
    margin-bottom: 1.5rem;
}

.list li {
    padding: 0.3rem 0 0.3rem 1rem;
    border-left: 2px solid var(--accent);
    margin-bottom: 0.5rem;
}

.japanese-text {
    background-color: #f0f0f0;
    padding: 1rem;
    margin: 1.5rem 0;
    font-size: 0.9rem;
    line-height: 1.8;
}

.social-links {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    margin-top: 1.5rem;
}

.social-link {
    text-decoration: none;
    color: var(--text-color);
    border: 2px solid var(--text-color);
    padding: 0.5rem 1rem;
    font-weight: 700;
    text-transform: uppercase;
    font-size: 0.8rem;
}

.social-link:hover {
    background-color: var(--text-color);
    color: var(--bg-color);
}

.inline-link {
    color: var(--accent);
    text-decoration: none;
    border-bottom: 1px solid var(--accent);
}

@media (max-width: 600px) {
    body {
        padding: 1rem;
    }

    .profile {
        flex-direction: column;
        align-items: flex-start;
    }

    .nav-links {
        flex-wrap: wrap;
    }
}

/* Hover effect styling */
.hover-container {
    position: relative;
    display: inline-block;
}

.hover-trigger {
    cursor: pointer;
}

.hover-box {
    position: absolute;
    display: none;
    width: 150px; 
    height: 150px; 
    z-index: 100;
    box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);
    border-radius: 5px;
    overflow: hidden;
    bottom: 30px;
    left: 50%;
    transform: translateX(-50%);
}

.hover-image {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.hover-container:hover .hover-box {
    display: block;
}



================================================
File: blog/decoding-transformers.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Decoding Transformers - Ayush's Blog</title>
        <meta name="description" content="An in-depth look at how Transformer models work, covering pretraining, batching, embeddings, attention mechanisms, and sampling strategies for LLMs." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="Decoding Transformers - Ayush's Blog" />
        <meta property="og:description" content="An in-depth look at how Transformer models work, covering pretraining, batching, embeddings, attention mechanisms, and sampling strategies for LLMs." />
        <meta property="og:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta property="og:url" content="https://goyalayus.github.io/blog/decoding-transformers.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <!-- Twitter Card data -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="Decoding Transformers - Ayush's Blog" />
        <meta name="twitter:description" content="An in-depth look at how Transformer models work, covering pretraining, batching, embeddings, attention mechanisms, and sampling strategies for LLMs." />
        <meta name="twitter:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta name="twitter:site" content="@goyalayus" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            :root {
                --bg-color: #f6f6f6;
                --text-color: #191919;
                --accent: #ff5500;
            }
            body {
                background-color: #ffffff;
                background-image: none;
                font-family: 'Space Mono', monospace;
                color: #333333;
                line-height: 1.6;
                padding: 0.5rem 1rem 2rem 1rem;
                max-width: 700px;
                margin: 0.5rem auto 2rem auto;
            }

            main {
                padding-top: 0;
            }

            .page-main-title {
                font-size: 2rem;
                font-weight: 700;
                color: #191919;
                margin-top: 0;
                margin-bottom: 1.5rem;
                border-bottom: none;
                text-transform: none;
                text-align: left;
            }

            .content-section h2 {
                font-size: 1.6rem;
                font-weight: 700;
                color: #191919;
                margin-top: 2.8rem;
                margin-bottom: 1.2rem;
                border-bottom: 1px solid #ccc;
                padding-bottom: 0.3rem;
                text-transform: none;
            }
            .content-section h2:first-of-type {
                margin-top: 1.5rem;
            }

            .content-section h3 {
                font-size: 1.3rem;
                font-weight: 700;
                color: var(--text-color);
                margin-top: 2rem;
                margin-bottom: 0.8rem;
                text-transform: none;
            }
            
            .content-section p {
                font-size: 1rem;
                margin-bottom: 1rem; /* Reduced for tighter paras */
                color: #333333;
            }
            .content-section p + p { /* Add a bit more space if one para follows another directly */
                margin-top: 0.5rem;
            }


            .content-section ul, .content-section ol {
                list-style-position: outside;
                margin-left: 20px;
                padding-left: 5px;
                margin-bottom: 1.5rem;
            }

            .content-section ul li, .content-section ol li {
                margin-bottom: 0.75rem;
                border-left: none;
                padding-left: 0;
            }
             .content-section ul li strong, .content-section ol li strong {
                font-weight: 700;
                color: var(--text-color);
            }
            
            .content-section a,
            .content-section a.inline-link {
                color: #007bff;
                text-decoration: underline;
            }

            .content-section a:hover,
            .content-section a.inline-link:hover {
                color: #0056b3;
            }

            .back-arrow {
                display: inline-block;
                margin-bottom: 1.5rem;
                font-size: 1rem;
                color: #007bff;
                text-decoration: none;
                font-weight: bold;
                transition: color 0.2s ease;
            }

            .back-arrow:hover {
                color: #0056b3;
                text-decoration: underline;
            }

            pre {
                background-color: #f0f0f0;
                padding: 1rem;
                margin-top: 0.8rem;
                margin-bottom: 1.5rem;
                overflow-x: auto;
                border: 1px solid #ddd;
                border-left: 4px solid var(--accent);
                font-size: 0.9em;
                line-height: 1.4;
                white-space: pre-wrap;
                word-wrap: break-word;
                border-radius: 4px;
            }
            pre code {
                font-family: 'Space Mono', monospace;
                background-color: transparent;
                padding: 0;
                border-radius: 0;
                border: none;
                font-size: inherit;
            }

            p code, li code {
                background-color: #e0e0e0;
                padding: 0.2em 0.4em;
                border-radius: 3px;
                font-size: 0.85em;
                font-family: 'Space Mono', monospace;
            }

            table {
                width: 100%;
                border-collapse: collapse;
                margin-bottom: 1.5rem;
                font-size: 0.9em;
            }
            th, td {
                border: 1px solid #ccc;
                padding: 0.5rem;
                text-align: left;
            }
            th {
                background-color: #f0f0f0;
                font-weight: bold;
            }
        </style>
    </head>
    <body>
        <main>
            <a href="https://goyalayus.github.io" class="back-arrow" aria-label="Go to homepage">⬅ Home</a>

            <section class="content-section">
                <h1 class="page-main-title">Decoding Transformers</h1>

                <h2>LLM Pretraining</h2>
                <p>
                    Transformers, at their core architecture, are basically next-word prediction machines.
                </p>
                <p>
                    You give it a phrase like "ayush is a good," and it will try to predict "boy." Depending on how close its prediction is to the actual next word, we calculate a loss and adjust the model's parameters.
                </p>
                <p>
                    The first step is to take your entire training data and convert each word (or token) into numbers using a tokenizer. Typically, about 90% of the words form the training data, and the remaining 10% become the test data.
                </p>

                <h2>Batching Mechanics: The <code>get_batch</code> Function</h2>
                <p><strong>Hyperparameters:</strong></p>
                <ul>
                    <li><code>batch_size=64</code> (sequences per batch)</li>
                    <li><code>block_size=256</code> (context window)</li>
                </ul>

                <p><strong>Visualization:</strong></p>
                <p>For <code>batch_size=3</code>, <code>block_size=5</code>:</p>
                <pre>
Original text positions: [0,1,2,3,4,5,6,7,8,...]
Batch 0: positions 2-6 → [2,3,4,5,6]
Batch 1: positions 5-9 → [5,6,7,8,9]
Batch 2: positions 9-13 → [9,10,11,12,13]
                </pre>
                <p><strong>Resulting Tensor Shape:</strong> <code>x.shape = (batch_size, block_size)</code> → (64, 256)</p>
                <p>
                    The <code>get_batch</code> function is very important. It provides random input and target batches for training and testing.
                </p>
                <p>
                    First, you check if it's training time or testing time and use the dataset accordingly. Suppose <code>batch_size</code> is 3, <code>block_size</code> is 5, and your training data (tokenized) is <code>[8,111,21,23,43,54,36,57,68,39,110,911]</code> (length 12).
                </p>
                <p>
                    The function will generate 3 (<code>batch_size</code>) random starting indices between 0 and (length of data - <code>block_size</code>), which is (12 - 5 = 7).
                </p>
                <p>
                    Suppose it chose indices 0, 2, and 5.
                </p>
                <p>
                    Your <code>x</code> input batch (input sequences) will be:
                </p>
                <pre>
[[  8, 111,  21,  23,  43],
 [ 21,  23,  43,  54,  36],
 [ 54,  36,  57,  68,  39]]
                </pre>
                <p>
                    And your <code>y</code> target batch (next word for each position in <code>x</code>) will be the +1 index shifted version of <code>x</code>:
                </p>
                <pre>
[[111,  21,  23,  43,  54],
 [ 23,  43,  54,  36,  57],
 [ 36,  57,  68,  39, 110]]
                </pre>

                <h2>Creating Embeddings</h2>
                
                <h3>PyTorch <code>nn.Embedding</code> - Simple Notes</h3>
                
                <h4>What is it?</h4>
                <ul>
                    <li>A layer that maps integer indices to dense vectors.</li>
                    <li>Used to convert word or item indices into learnable embeddings.</li>
                </ul>

                <h4>Syntax</h4>
                <pre><code class="language-python">
import torch.nn as nn

nn.Embedding(num_embeddings, embedding_dim)
                </code></pre>
                <ul>
                    <li><code>num_embeddings</code>: total number of unique indices (e.g., vocabulary size)</li>
                    <li><code>embedding_dim</code>: size of each embedding vector</li>
                </ul>

                <h4>What does <code>num_embeddings = 10</code> mean?</h4>
                <ul>
                    <li>It defines that there are 10 distinct items (e.g., words).</li>
                    <li>It creates an internal lookup table of shape (10, <code>embedding_dim</code>).</li>
                    <li>Valid input indices range from 0 to 9.</li>
                </ul>

                <h4>Example</h4>
                <p><code>embedding = nn.Embedding(10, 3)</code> creates something like:</p>
                <table>
                    <thead>
                        <tr><th>Index</th><th>Vector (3D)</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>0</td><td>[0.12, -0.45, 0.78] (example)</td></tr>
                        <tr><td>1</td><td>[0.34, 0.91, -0.67] (example)</td></tr>
                        <tr><td>...</td><td>...</td></tr>
                        <tr><td>9</td><td>[0.01, 0.22, 0.93] (example)</td></tr>
                    </tbody>
                </table>
                <pre><code class="language-python">
import torch
import torch.nn as nn

embedding = nn.Embedding(10, 3)
input_indices = torch.LongTensor([1, 2, 4, 5])
output_vectors = embedding(input_indices)
print(output_vectors)
                </code></pre>
                <p>Example output:</p>
                <pre>
tensor([[ 0.6614,  0.2669,  0.0617],
        [ 0.6213, -0.4519, -0.1661],
        [-0.3727, -0.4709,  0.1994],
        [ 0.1008,  0.2113,  0.3170]], grad_fn=<EmbeddingBackward0>)
                </pre>
                <p>The output shape is (4, 3) — 4 indices, each mapped to a 3D vector.</p>

                <h4>Initialization</h4>
                <p>Embedding vectors are randomly initialized.</p>

                <h4>Trainable?</h4>
                <p>Yes. These vectors are learnable and updated via backpropagation.</p>

                <p>Let's now use these <code>nn.Embeddings</code>.</p>
                <pre><code class="language-python">
self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
self.position_embedding_table = nn.Embedding(block_size, n_embd)
                </code></pre>
                <p>
                    <code>n_embd</code> is the size of embeddings we wish; generally, it's around 768 (e.g. for GPT-2 base). A question may arise here: why can't we use one-hot vectors? The reason is they become very sparse. Suppose you have 45k vocab words; then it would be a 45k-length array with all indices 0 except 1.
                </p>
                <p>
                    <code>vocab_size</code> is the total vocabulary in the tokenizer (generally around 50k for models like GPT-2).
                </p>

                <h3>Forward Pass Through Embeddings</h3>
                <p>In the <code>forward</code> method:</p>
                <pre><code class="language-python">
tok_emb = self.token_embedding_table(idx)
pos_emb = self.position_embedding_table(torch.arange(T, device=device))
x = tok_emb + pos_emb
                </code></pre>
                <ul>
                    <li>
                        <strong><code>tok_emb</code>:</strong>
                        <ul>
                            <li>Input: <code>idx</code> of shape (B, T) → Batch of token indices.</li>
                            <li>Output: Dense embeddings for each token. Shape: (B, T, C)</li>
                            <li>Example (for B=2, T=2, C=2):
                                <pre>
idx = [[2, 5], [1, 3]]
tok_emb = [[[0.1, 0.2], [0.3, 0.4]],
           [[0.5, 0.6], [0.7, 0.8]]]
                                </pre>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong><code>pos_emb</code>:</strong>
                        <ul>
                            <li>Input: Positions (e.g., <code>torch.arange(T)</code>) for each token in the sequence. Shape: (T)</li>
                            <li>Output: Dense embeddings for each position. Shape: (T, C)</li>
                            <li>Example (for T=2, C=2):
                                <pre>
pos_emb = [[0.01, 0.02],
           [0.03, 0.04]]
                                </pre>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <strong><code>x</code>:</strong>
                        <ul>
                            <li>Combines token and positional embeddings by adding them element-wise. Positional embeddings (T, C) are broadcast across the batch dimension (B) to match token embeddings (B, T, C). Shape: (B, T, C)</li>
                            <li>Example:
                                <pre>
x = tok_emb + pos_emb
  = [[[0.11, 0.22], [0.33, 0.44]],
     [[0.51, 0.62], [0.73, 0.84]]]
                                </pre>
                            </li>
                        </ul>
                    </li>
                </ul>

                <h2>The Transformer Block</h2>
                <p>Let's move forward to a Transformer block's code.</p>
                <pre><code class="language-python">
import torch.nn as nn
import torch.nn.functional as F # Assuming F is used later

class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_embd, n_head, head_size) # Assuming MultiHeadAttention takes n_embd too
        self.ffwd = FeedForward(n_embd) # Assuming FeedForward class definition
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x
                </code></pre>
                <p>An example instantiation:</p>
                <pre><code class="language-python">
# block = Block(n_embd=768, n_head=12) # Example for a typical setup
# output = block(x_input_embeddings)
                </code></pre>
                <p>
                    Now, I assume that you are a little bit familiar with how object-oriented programming works.
                </p>
                <p>
                    You do not need to care about what <code>MultiHeadAttention</code> or <code>FeedForward</code> classes are as of now. You just need to know that when you run this particular line <code>block = Block(n_embd_val, n_head_val)</code>, the <code>__init__</code> method:
                </p>
                <pre><code class="language-python">
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_embd, n_head, head_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
                </code></pre>
                <p>
                    ...runs, and you have all these attributes initialized. For example, <code>block.ffwd</code> is an object of class <code>FeedForward</code>.
                </p>
                <p>
                    When <code>self.ffwd = FeedForward(n_embd)</code> this code runs, the <code>__init__</code> method inside this <code>FeedForward</code> class also runs and helps create attributes for the object (<code>self.ffwd</code>). You do not need to know how that <code>__init__</code> method looks as of now.
                </p>
                <p>
                    And when you run <code>output = block(x)</code>, because the <code>Block</code> class inherits from <code>nn.Module</code>, the <code>forward</code> method will automatically run with the execution of this line. So:
                </p>
                <pre><code class="language-python">
    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x
                </code></pre>
                <p>
                    First, this line <code>x = x + self.sa(self.ln1(x))</code> would run. Let's break this down.
                </p>
                <p>
                    <code>self.ln1(x)</code> means the <code>forward</code> method of this <code>ln1</code> object (LayerNorm) will run with input <code>x</code>.
                </p>
                <p>
                    Similarly, for <code>self.sa(self.ln1(x))</code>, the <code>forward</code> method of the <code>self.sa</code> object (MultiHeadAttention) will run with <code>self.ln1(x)</code> as input. The same logic applies to the line below it for the feedforward network.
                </p>
                <p>
                    Now, some of you might be wondering what <code>nn.LayerNorm(n_embd)</code> does. Basically, it takes your input <code>x</code> and normalizes it across the embedding dimension for each token.
                </p>

                <h2>Multi-Head Self-Attention In-depth</h2>
                <p>
                    So now let's explore the line <code>x = x + self.sa(self.ln1(x))</code> in depth, focusing on the <code>MultiHeadAttention</code> part.
                </p>
                <p>
                    Currently, each token has a dimension of <code>n_embd</code>. Now, I want you to remember, and just remember for now, that there is a process called *attention*. One important thing to note about attention is we pass it the matrix (basically our input <code>x</code>) of dimension (batch size, block size, <code>n_embd</code>), and it does some calculations on that matrix and outputs it with dimension (batch size, block size, <code>head_size</code>).
                </p>
                <p>
                    Remember, <code>head_size = n_embd // n_head</code>.
                </p>
                <p>
                    So, what multi-head attention does is, it creates (number of heads) * attention processes (or "heads"). You input (batch size, block size, <code>n_embd</code>) and get (number of heads) matrices, each of (batch size, block size, <code>head_size</code>).
                </p>
                <p>
                    Suppose number of heads is 3, <code>n_embd = 9</code>, so <code>head_size = 3</code>. An input token embedding <code>[2,4,3,5,8,6,7,3,1]</code> (9-dim) would conceptually result in 3 matrices (3-dim each from each head), for example: <code>[21,34,54]</code>, <code>[1.2,5.4,32]</code>, <code>[2.3,6.1,.9]</code>.
                </p>
                <p>
                    Then what we do is concatenate all these head outputs: <code>[21,34,54,1.2,5.4,32,2.3,6.1,0.9]</code> (back to 9-dim) and usually pass it through a final linear projection.
                </p>
                <p>
                    In the line <code>x = x + self.sa(self.ln1(x))</code>, the output of <code>self.sa(...)</code> (the multi-head attention mechanism) will have the same dimension <code>n_embd</code> as the input <code>x</code>.
                </p>

                <h2>The Attention Mechanism (Single Head)</h2>
                <p>Now let's move to this attention thing, what does this do?</p>
                <pre><code class="language-python">
class Head(nn.Module):
    def __init__(self, n_embd, head_size, block_size, dropout_val):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        self.dropout = nn.Dropout(dropout_val)

    def forward(self, x):
        B, T, C_in = x.shape
        k = self.key(x)
        q = self.query(x)
        
        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5
        
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        
        v = self.value(x)
        out = wei @ v
        return out
                </code></pre>
                <p>
                    <code>k = self.key(x)</code> and <code>q = self.query(x)</code> both take <code>x</code> (with dimension B, T, <code>n_embd</code>) and produce matrices K and Q respectively, each with dimension (B, T, <code>head_size</code>) via linear transformations.
                </p>
                <p>
                    Now, in the line <code>wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5</code>, we calculate the scaled dot-product attention scores. <code>q @ k.transpose(-2, -1)</code> performs a matrix multiplication between queries (Q) and transposed keys (K<sup>T</sup>), resulting in a matrix <code>wei</code> of dimension (B, T, T). If you remember, <code>block_size</code> (T) was the number of tokens we are processing at once.
                </p>
                <p>
                    So now, let us understand the philosophy behind attention. The meaning of words is different in different contexts. For example, consider these two sentences:
                </p>
                <ol>
                    <li>This bag is <strong>light</strong>.</li>
                    <li>I turned on the <strong>light</strong>.</li>
                </ol>
                <p>
                    The word is the same, "light," but the meaning is totally different. So how do we capture this meaning via the attention mechanism? You see that <code>wei</code> matrix; for one batch item, it has dimensions (T, T) or (block_size, block_size). What this does is calculate the importance of each token with respect to every other token in the sequence.
                </p>
                <p>Conceptually, for a sequence "this is a good boy" (T=5), <code>wei</code> before masking and softmax might look like:</p>
                <table>
                    <thead><tr><th>Query Token</th><th>Key: this</th><th>Key: is</th><th>Key: a</th><th>Key: good</th><th>Key: boy</th></tr></thead>
                    <tbody>
                        <tr><td>this</td><td>0.98</td><td>0.62</td><td>0.47</td><td>0.33</td><td>0.21</td></tr>
                        <tr><td>is</td><td>0.61</td><td>0.97</td><td>0.56</td><td>0.45</td><td>0.29</td></tr>
                        <tr><td>a</td><td>0.44</td><td>0.59</td><td>0.96</td><td>0.52</td><td>0.34</td></tr>
                        <tr><td>good</td><td>0.32</td><td>0.49</td><td>0.53</td><td>0.95</td><td>0.68</td></tr>
                        <tr><td>boy</td><td>0.25</td><td>0.33</td><td>0.37</td><td>0.66</td><td>0.94</td></tr>
                    </tbody>
                </table>
                <p>
                    So here you can see the <code>block_size</code> is 5. Now, one thing to note is how we train this transformer for tasks like next-word prediction.
                </p>
                <p>
                    I will first give it the word "this" and ask it to predict the next word, which is "is". Then I will give it "this is" and ask it to predict the next word, which is "a".
                </p>
                <p>
                    Now, for a given token to predict the *next* token, you do not want the model to "see" or know about future tokens in the sequence (e.g., when processing "is", it shouldn't know "a" comes after). Otherwise, it would be cheating. So, we *mask* it.
                </p>
                <p>Here is how the masked version of <code>wei</code> (before softmax) looks:</p>
                <table>
                    <thead><tr><th>Query Token</th><th>Key: this</th><th>Key: is</th><th>Key: a</th><th>Key: good</th><th>Key: boy</th></tr></thead>
                    <tbody>
                        <tr><td>this</td><td>0.98</td><td>-∞</td><td>-∞</td><td>-∞</td><td>-∞</td></tr>
                        <tr><td>is</td><td>0.61</td><td>0.97</td><td>-∞</td><td>-∞</td><td>-∞</td></tr>
                        <tr><td>a</td><td>0.44</td><td>0.59</td><td>0.96</td><td>-∞</td><td>-∞</td></tr>
                        <tr><td>good</td><td>0.32</td><td>0.49</td><td>0.53</td><td>0.95</td><td>-∞</td></tr>
                        <tr><td>boy</td><td>0.25</td><td>0.33</td><td>0.37</td><td>0.66</td><td>0.94</td></tr>
                    </tbody>
                </table>
                <p>
                    Now you might be wondering why I used negative infinity (<code>-inf</code>) instead of 0. Wait, let me show you. So now, as you can see, when the model is processing the token "is" (as a query), it can attend to "this" and "is" (as keys), but its attention to "a", "good", and "boy" is blocked (set to <code>-inf</code>).
                </p>
                <p>
                    Now what we do is apply softmax to this masked <code>wei</code> matrix (<code>F.softmax(wei, dim=-1)</code>). This converts the scores in each row into a probability distribution, meaning the sum of attention weights in each row will be 1.
                </p>
                <p>So after applying softmax, it looks like (attention weights):</p>
                <table>
                    <thead><tr><th>Query Token</th><th>Attends to: this</th><th>Attends to: is</th><th>Attends to: a</th><th>Attends to: good</th><th>Attends to: boy</th></tr></thead>
                    <tbody>
                        <tr><td>this</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr>
                        <tr><td>is</td><td>0.36</td><td>0.64</td><td>0.00</td><td>0.00</td><td>0.00</td></tr>
                        <tr><td>a</td><td>0.24</td><td>0.39</td><td>0.37</td><td>0.00</td><td>0.00</td></tr>
                        <tr><td>good</td><td>0.19</td><td>0.26</td><td>0.28</td><td>0.27</td><td>0.00</td></tr>
                        <tr><td>boy</td><td>0.15</td><td>0.19</td><td>0.20</td><td>0.25</td><td>0.21</td></tr>
                    </tbody>
                </table>
                <p>
                    Now, <code>wei</code> is of dimension (B, T, T), but we need to get an output of dimension (B, T, <code>head_size</code>) for this attention head.
                </p>
                <p>
                    So we multiply it with the Value matrix (<code>v = self.value(x)</code>, which has shape B, T, <code>head_size</code>). The operation is <code>out = wei @ v</code>. This results in <code>out</code> with shape (B, T, <code>head_size</code>), which is a weighted sum of the value vectors.
                </p>
                <p>
                    Now you might be wondering why we use multiple attention heads. What's wrong with using one attention? We use multiple attention heads to allow the model to capture different types of relationships or "sentiments" simultaneously. Some heads might capture what words are emotionally more important, while other attention heads might capture grammatical superiority, and so on.
                </p>
                
                <h2>FeedForward Network (FFN)</h2>
                <p>
                    So after passing our input <code>x</code> (B, T, <code>n_embd</code>) through this multi-head attention mechanism (which includes concatenating head outputs and a final linear projection), we get a richer meaning output <code>x</code> (still with dimensions B, T, <code>n_embd</code> after the residual connection).
                </p>
                <p>
                    Now we pass this through a position-wise feedforward neural network (FFN). The FFN typically consists of two linear transformations with a non-linear activation function in between. A common structure is:
                </p>
                <ul>
                    <li>Linear Layer 1: Projects the input from dimension <code>n_embd</code> to a higher dimension, often 4 times larger (4 * <code>n_embd</code>).</li>
                    <li>Non-linear Activation: Applies a non-linear function like ReLU or GELU.</li>
                    <li>Linear Layer 2: Projects the output back to the original dimension <code>n_embd</code>.</li>
                </ul>
                <p>
                    This whole sequence (Multi-Head Attention + Add & Norm -> Feedforward + Add & Norm) is called a Transformer block. And we pass our <code>x</code> input through multiple such blocks, one after another:
                </p>
                <pre>
x → block 1 → x' → block 2 → x'' .....
                </pre>

                <h2>Final Output and Loss Calculation</h2>
                <p>Now let's move ahead to the model's main <code>forward</code> method, which orchestrates these components.</p>
                <pre><code class="language-python">
class YourTransformerModel(nn.Module):
    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd) 
        self.lm_head = nn.Linear(n_embd, vocab_size)
        self.dropout = nn.Dropout(dropout) 

    def forward(self, idx, targets=None):
        B, T = idx.shape

        tok_emb = self.token_embedding_table(idx) 
        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))
        x = self.dropout(tok_emb + pos_emb)
        
        x = self.blocks(x) 
        
        x = self.ln_f(x) 

        if targets is not None:
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            logits = self.lm_head(x[:, [-1], :])
            loss = None
            
        return logits, loss
                </code></pre>
                <p>
                    The initial lines for token and positional embeddings, their summation, and dropout are what we've discussed. The <code>self.blocks(x)</code> line passes the combined embeddings through the stack of Transformer blocks.
                </p>
                <p>
                    The <code>def forward(self, idx, targets=None):</code> method takes input token indices <code>idx</code> (shape B, T) and optional <code>targets</code>.
                </p>
                <p>
                    <code>x = self.ln_f(x)</code> applies a final layer normalization to the output of the Transformer blocks.
                </p>
                <p>
                    The line <code>if targets is not None:</code> means if we have targets (which we calculated at the start for training), we are in training mode (not inference mode like when you use ChatGPT).
                </p>
                <p>
                    Then, <code>logits = self.lm_head(x)</code> applies a linear layer (<code>lm_head</code>) that takes the processed <code>x</code> (B, T, <code>n_embd</code>) and projects it to (B, T, <code>vocab_size</code>). These are the raw scores for each possible next token.
                </p>
                <p>
                    Now, if you remember, <code>vocab_size</code> is the set of all possible tokens. So, for each token in the input sequence, <code>logits</code> gives scores for every token in the vocabulary as a potential successor. For an input "ayush is good", for the representation corresponding to "ayush", we want the logit for "is" to be high. The model is trained accordingly.
                </p>
                <p>
                    <code>loss = F.cross_entropy(...)</code> then calculates the cross-entropy loss between these predicted <code>logits</code> and the actual <code>targets</code> (e.g., "is good boy"). I am not covering cross-entropy loss here; you can google it or ask ChatGPT.
                </p>
                <p>
                    The <code>else:</code> block handles inference mode.
                </p>
                <p>
                    <code>logits = self.lm_head(x[:, [-1], :])</code> means that if you are in inference mode, do not calculate the loss. Instead, just give the score of the next word for the *last* word in the input sequence. If "ayush is good" is the input (and <code>block_size</code> allows for it), we are asking the model to give scores for what will come after "good".
                </p>
                <p>
                    The expression <code>x[:, [-1], :]</code> works as follows:
                </p>
                <ul>
                    <li><code>x</code> has shape (B, T, <code>n_embd</code>) — batch size, sequence length, embedding dimension.</li>
                    <li><code>x[:, [-1], :]</code> selects the last token's embedding (at index T-1) in the sequence for each batch item.</li>
                    <li>The shape after indexing is (B, 1, <code>n_embd</code>):
                        <ul>
                            <li><code>:</code> → all batches</li>
                            <li><code>[-1]</code> → last token position (kept as a dimension of size 1)</li>
                            <li><code>:</code> → all embedding dimensions</li>
                        </ul>
                    </li>
                </ul>

                <h2>Sampling Strategies in Inference Time</h2>
                
                <h3>1. What Are Sampling Strategies?</h3>
                <p>
                    When generating text, the model produces a vector of logits (unnormalized scores) for every possible next token. To turn these into an actual token choice, you need a sampling strategy:
                </p>
                <ul>
                    <li><strong>Greedy Decoding:</strong> Always pick the token with the highest probability (least random).</li>
                    <li><strong>Random Sampling:</strong> Pick tokens according to their probability distribution (more creative).</li>
                    <li><strong>Temperature & Top-k:</strong> Methods to control the balance between randomness and determinism.</li>
                </ul>

                <h3>2. Temperature</h3>
                <h4>What is it?</h4>
                <p>A scalar value that controls the "sharpness" or "flatness" of the probability distribution.</p>
                <h4>How is it used?</h4>
                <pre><code class="language-python">
logits = logits / temperature
probs = F.softmax(logits, dim=-1)
                </code></pre>
                <ul>
                    <li>Low temperature (<1): Makes the distribution sharper (model is more confident, less random).</li>
                    <li>High temperature (>1): Flattens the distribution (model is more random, more creative).</li>
                </ul>
                <h4>Example:</h4>
                <p>If logits = <code>[2.0, 1.0, 0.1]</code>:</p>
                <ul>
                    <li>With temperature 1.0: softmax is "normal."</li>
                    <li>With temperature 0.5: logits become <code>[4.0, 2.0, 0.2]</code> → softmax is sharper.</li>
                    <li>With temperature 2.0: logits become <code>[1.0, 0.5, 0.05]</code> → softmax is flatter.</li>
                </ul>

                <h3>3. Top-k Sampling</h3>
                <h4>What is it?</h4>
                <p>A technique to restrict sampling to only the top <code>k</code> most likely tokens.</p>
                <h4>How is it used?</h4>
                <pre><code class="language-python">
v, _ = torch.topk(logits, k)
logits[logits < v[:, [-1]]] = -float('Inf')
probs = F.softmax(logits, dim=-1)
                </code></pre>
                <ul>
                    <li>Only the <code>k</code> tokens with the highest logits are kept; all others are set to <code>-inf</code> (zero probability after softmax).</li>
                    <li>The next token is sampled only from these top <code>k</code> candidates.</li>
                </ul>
                <h4>Why use it?</h4>
                <ul>
                    <li>Prevents the model from picking rare, low-probability tokens that can lead to incoherent or nonsensical text.</li>
                    <li>Balances creativity and coherence.</li>
                </ul>

                <h3>4. Combined Effect</h3>
                <ul>
                    <li>Temperature controls the overall randomness.</li>
                    <li>Top-k controls the candidate set for each prediction.</li>
                    <li>Used together, you can finely tune the model’s output: from deterministic and repetitive to creative and surprising.</li>
                </ul>

                <h2>Conclusion</h2>
                <p>
                    If you have ever heard of "context window," it is the same as <code>block_size</code>.
                </p>
                <p>
                    This majorly sums up the Transformer architecture. For queries, hit me up at <a href="https://x.com/goyalayus" target="_blank" rel="noopener noreferrer">@goyalayus</a>.
                </p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/distribution-for-pmf.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Distribution for PMF - Ayush's Blog</title>
        <meta name="description" content="Exploring the advantages and nuances of having distribution for achieving Product-Market Fit, and how founders can leverage it." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="Distribution for PMF - Ayush's Blog" />
        <meta property="og:description" content="Exploring the advantages and nuances of having distribution for achieving Product-Market Fit, and how founders can leverage it." />
        <meta property="og:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta property="og:url" content="https://goyalayus.github.io/blog/distribution-for-pmf.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            body {
                background-color: #ffffff;
                background-image: none;
                font-family: 'Space Mono', monospace;
                color: #333333;
                line-height: 1.6;
                padding: 0.5rem 1rem 2rem 1rem;
                max-width: 700px;
                margin: 0.5rem auto 2rem auto;
            }

            main {
                padding-top: 0;
            }

            .page-main-title {
                font-size: 2rem;
                font-weight: 700;
                color: #191919;
                margin-top: 0;
                margin-bottom: 1.5rem;
                border-bottom: none;
                text-transform: none;
                text-align: left;
            }

            .content-section h2 {
                font-size: 1.4rem;
                font-weight: 700;
                color: #191919;
                margin-top: 2.5rem;
                margin-bottom: 1rem;
                border-bottom: none;
                text-transform: none;
            }

            .content-section p {
                font-size: 1rem;
                margin-bottom: 1rem;
                color: #333333;
            }

            .content-section ul {
                list-style: disc;
                margin-left: 20px;
                padding-left: 5px;
                margin-bottom: 1.5rem;
            }

            .content-section ul li {
                margin-bottom: 0.75rem;
                border-left: none;
                padding-left: 0;
            }

            .content-section ul li strong {
                font-weight: 700;
            }

            .content-section a,
            .content-section a.inline-link {
                color: #007bff;
                text-decoration: underline;
            }

            .content-section a:hover,
            .content-section a.inline-link:hover {
                color: #0056b3;
            }

            .back-arrow {
                display: inline-block;
                margin-bottom: 1rem;
                font-size: 1rem;
                color: #007bff;
                text-decoration: none;
                font-weight: bold;
                transition: color 0.2s ease;
            }

            .back-arrow:hover {
                color: #0056b3;
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <main>
            <a href="https://goyalayus.github.io" class="back-arrow" aria-label="Go to homepage">⬅ Home</a>

            <section class="content-section">
                <h1 class="page-main-title">Distribution for PMF</h1>

                <p>Founders who have distribution have a real advantage but not in the way people think about it.</p>

                <p>As a founder, your first startup almost never works out. You try a dozen ideas and maybe one works out.</p>

                <p>But what do I mean by trying out an idea? It means building the product, reaching your niche audience (100 people who absolutely love your product), taking feedback from these and finding product-market fit (basically high retention on product).</p>

                <p>Most founders struggle with finding these <em>100 people who have the potential to love your product</em>, for whom you were building your product.</p>

                <p>Now you can reach them in three ways:</p>
                <ul>
                    <li><strong>Manually</strong> - you cold email people, try to get intros, physical marketing.</li>
                    <li><strong>You burn investor money to buy distribution</strong> - paid sponsorships / ads.</li>
                    <li><strong>You have your own distribution</strong> - suppose you have 10k followers on Twitter. Now you can shitpost about your product and tell other people to repost your product post (10k followers gives you credibility). Also, most people on tech Twitter are early adopters. So your <em>speed to market</em> increases a lot, and you are able to do many experiments and get to PMF fast.</li>
                </ul>

                <p>But this has a flaw. The flaw is, suppose you were building a dev tool and your Twitter following came in massive help there, but you did not get PMF. Now you get an idea to start a construction software. None of your followers belong to this space. Now what do you do? Maybe you ask someone from the construction space to repost your post, and yeah, I think this will work to at least get some 100 people, but you can't do this repeatedly. Not every post of yours your friend is gonna retweet.</p>

                <p>But this is even more complicated. For example, you make content around college students who want to startup, and your product is around corporate people who want to startup, then also you end up in the same problem. Retweets are easier in this case but still.</p>

                <p>One solution to this is I think to become something like <a href="https://x.com/waitin4agi_" target="_blank" rel="noopener noreferrer">Varun Mayya</a>. Get to a million followers on YouTube. Then suppose you are building something for Tier-1 city nurses, for example, you just have to post a tweet/YouTube post about rolling it out to early beta testers and boom you have 1000 <em>niche</em> and <em>targeted users using your product</em>, much quicker feedback.</p>

                <p>But building this massive distribution takes years. So another simple way is to pay a very niche but very strong cult-following Twitter account (1–10k followers). But you need to have some money to do this — either savings or funding.</p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/how-llm-tokenizers-work-bpe.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>How LLM Tokenizers Work: Understanding Byte-Pair Encoding (BPE) - Ayush's Blog</title>
        <meta name="description" content="ChatGPT doesn't just 'understand' English; it processes numbers. This post explains how tokenizers, especially Byte-Pair Encoding (BPE), convert your text into a format LLMs can work with." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="How LLM Tokenizers Work: Understanding BPE - Ayush's Blog" />
        <meta property="og:description" content="ChatGPT doesn't just 'understand' English; it processes numbers. This post explains how tokenizers, especially Byte-Pair Encoding (BPE), convert your text into a format LLMs can work with." />
        <meta property="og:image" content="https://goyalayus.github.io/blog-og-tokenizer.png" /> <!-- REPLACE THIS WITH AN ACTUAL IMAGE URL -->
        <meta property="og:url" content="https://goyalayus.github.io/blog/how-llm-tokenizers-work-bpe.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <!-- Twitter Card data -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="How LLM Tokenizers Work: Understanding BPE - Ayush's Blog" />
        <meta name="twitter:description" content="ChatGPT doesn't just 'understand' English; it processes numbers. This post explains how tokenizers, especially Byte-Pair Encoding (BPE), convert your text into a format LLMs can work with." />
        <meta name="twitter:image" content="https://goyalayus.github.io/blog-og-tokenizer.png" /> <!-- REPLACE THIS WITH AN ACTUAL IMAGE URL -->
        <meta name="twitter:site" content="@goyalayus" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            main section h2 { /* Subheadings like "Flawed Approaches" */
                font-size: 1.3rem;
                margin-top: 2.5rem; /* Increased top margin */
                margin-bottom: 1.25rem; /* Increased bottom margin */
                text-transform: none;
                border-bottom: 1px solid var(--text-color);
                display: inline-block;
            }

            /* Styling for the main ordered list (e.g., "Flawed Approaches") */
            main section > ol.list {
                list-style-type: decimal;
                list-style-position: outside;
                margin-left: 1.5rem; 
                margin-bottom: 1.5rem;
            }
            main section > ol.list > li {
                border-left: none; /* Override default from style.css */
                padding-left: 0.5rem; /* Space after number */
                margin-bottom: 1.25rem; /* Increased space between items */
                line-height: 1.6;
            }
             main section > ol.list > li strong { /* Make the title of list item bolder if needed */
                font-weight: 700;
            }


            .content-block { /* For the BPE example box */
                background-color: #f0f0f0;
                padding: 1.5rem;
                margin-top: 1rem; /* Space above the block */
                margin-bottom: 2rem; /* Space below the block */
                overflow-x: auto;
                border: 1px solid #ddd;
                border-left: 4px solid var(--accent);
                font-size: 0.9em;
                border-radius: 4px;
                line-height: 1.6;
            }
            .content-block p {
                margin-bottom: 1rem; 
            }
            .content-block p:last-child {
                margin-bottom: 0;
            }
            .content-block p strong { /* For labels like "Sentence:", "Iteration 1:" */
                color: var(--text-color); 
                font-weight: 700;
            }
            .content-block pre {
                white-space: pre-wrap;
                word-wrap: break-word;
                background-color: #e9e9e9; 
                padding: 0.75rem 1rem;
                margin-top: 0.5rem; 
                margin-bottom: 1rem; 
                border-radius: 3px;
                font-size: 0.95em; 
                line-height: 1.5;
            }
            .content-block ol {
                list-style-type: decimal;
                list-style-position: outside; 
                margin-left: 1.5rem; 
                margin-top: 0.5rem;
                margin-bottom: 1.25rem; /* Increased space */
            }
            .content-block ol > li {
                padding-left: 0.5rem; 
                margin-bottom: 1rem; /* Increased space */
            }
            .content-block ol > li:last-child {
                margin-bottom: 0;
            }
            .content-block ol pre { 
                margin-top: 0.5rem;
            }
        </style>
    </head>
    <body>
        <header>
            <div class="profile">
                <a href="../index.html">
                    <img
                        src="../ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="../index.html" class="nav-link">ABOUT</a>
                <a href="../blog.html" class="nav-link active">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h1 class="section-title" style="font-size: 1.5rem; text-transform: none; border-bottom-width: 2px; margin-bottom: 1.5rem;">How LLM Tokenizers Work: Understanding Byte-Pair Encoding (BPE)</h1>
                
                <p class="text">ChatGPT doesn't understand your English words directly; it only understands numbers. So whenever you prompt something into it, it first generates numbers associated with each word (or part of a word) and then processes those numbers.</p>
                <p class="text">For example, the sentence "he ate a crocodile." might be converted into a sequence of numbers like <code>[234, 412, 232, 767]</code>.</p>
                <p class="text">The algorithm responsible for this conversion—turning words into numbers—is called a tokenizer.</p>

                <h2>Flawed Approaches to Tokenization</h2>
                <p class="text">If you first think about how to design a tokenizer, some basic approaches come to mind, but they have significant issues:</p>
                <ol class="list">
                    <li>
                        <strong>Assign a number to each character:</strong> Instead of assigning a number to a whole word, we could assign one to each character (e.g., a → 1, b → 2).
                        <br/>For example: "he ate a crocodile" → <code>['h','e',' ','a','t','e',' ','a',' ','c','r','o','c','o','d','i','l','e']</code> → <code>[8, 5, 19, 1, 14, 5, 19, 1, 19, 3, 12, 11, 3, 11, 4, 9, 10, 5]</code> (hypothetical mapping).
                        <br/>The issue with this is that the input sequences become very long, making processing slow and costly. Also, individual characters often don't carry enough semantic meaning on their own.
                    </li>
                    <li>
                        <strong>Assign each word in the world a specific number:</strong> Doing this is very hard because the number of unique words in any language is vast (practically infinite if you consider all variations, typos, new words, etc.). This would lead to an enormous vocabulary and other problems like handling unknown words.
                    </li>
                </ol>

                <h2>The Optimal Solution: Byte-Pair Encoding (BPE)</h2>
                <p class="text">So, what's the optimal solution? Many modern LLMs use a technique called Byte-Pair Encoding (BPE).</p>
                <p class="text">LLMs are trained on huge amounts of text data from the internet. The BPE algorithm also learns from this data. Here's the core idea:
                You take the training data, and for a pre-decided number of iterations (e.g., 30,000 to 50,000), you repeat the following steps:
                Find the pair of characters (or existing tokens) that appears most frequently together in the data.
                Merge this pair into a single new token.
                Add this new token to your vocabulary.</p>
                <p class="text">This way, common words (like "the", "is") or common sub-word units (like "ing", "est", "token") eventually become single tokens in the vocabulary, while rare words can still be represented as a sequence of smaller, known sub-word tokens.</p>

                <p class="text">Let's look at a simplified, illustrative example of how BPE might work on a small piece of text. The following is a conceptual walkthrough:</p>
                
                <div class="content-block">
                    <p>Okay, let's run two iterations of BPE on the given sentence, with minimal explanation to emulate the algorithmic steps:</p>

                    <p><strong>Sentence:</strong> "The sun rose quietly over the hills, casting golden light across the valley as birds chirped softly, welcoming the calm beauty of a new day."</p>

                    <p><strong>Initial Tokens (with end-of-word markers like <code></w></code>, and assuming basic tokenization where words are units):</strong></p>
                    <pre>"The</w>", "sun</w>", "rose</w>", "quietly</w>", "over</w>", "the</w>", "hills</w>", "casting</w>", "golden</w>", "light</w>", "across</w>", "the</w>", "valley</w>", "as</w>", "birds</w>", "chirped</w>", "softly</w>", "welcoming</w>", "the</w>", "calm</w>", "beauty</w>", "of</w>", "a</w>", "new</w>", "day</w>"</pre>
                    <p>(Note: BPE often starts from individual characters of the training corpus. For this example, we're simplifying the starting point.)</p>

                    <p><strong>Initial Vocabulary (unique characters from the sentence, plus the end-of-word marker):</strong></p>
                    <pre>{'T', 'h', 'e', '</w>', 's', 'u', 'n', 'r', 'o', 'q', 'i', 't', 'l', 'y', 'v', 'c', 'a', 'g', 'd', 'p', 'f', 'm', 'b', 'k', 'w' ... (all unique characters)}</pre>

                    <p><strong>Iteration 1:</strong></p>
                    <ol>
                        <li><strong>Count Adjacent Pairs:</strong> (Example: "T" "h", "h" "e", "e" "</w>", etc., across the whole corpus)</li>
                        <li><strong>Most Frequent Pair:</strong> Let's say, hypothetically, after counting, the most frequent pair is ("e", "</w>").</li>
                        <li><strong>Merge:</strong> Create new token "e</w>".</li>
                        <li><strong>Updated Tokens (Conceptual):</strong> Words ending in "e" would now use this new token. For example, "rose</w>" could be seen as <code>r o s e</w></code>. The representation in the text changes.
                            <pre>"Th e</w>", "sun</w>", "ros e</w>", "quietly</w>", "over</w>", "th e</w>", "hills</w>", ...</pre>
                        </li>
                        <li><strong>Updated Vocabulary:</strong> Now includes "e</w>" as a token.
                            <pre>{..., 'e</w>'}</pre>
                        </li>
                    </ol>

                    <p><strong>Iteration 2:</strong></p>
                    <ol>
                        <li><strong>Count Adjacent Pairs:</strong> (Now including "e</w>" as a potential part of a pair) Let's say the most frequent is ("t", "h").</li>
                        <li><strong>Merge:</strong> Create new token "th".</li>
                        <li><strong>Updated Tokens (Conceptual):</strong> Sequences of "th" would now use this token.
                            <pre>"Th e</w>", "sun</w>", "ros e</w>", "quietly</w>", "over</w>", "th e</w>", "hills</w>", ... </pre>
                            (Note: "The" might become "Th" + "e</w>" if "Th" was already a token or became one. If "th" is a new token, then "the" as <code>t h e</w></code> might become <code>th e</w></code>.)
                        </li>
                        <li><strong>Updated Vocabulary:</strong> Now includes "th" as a token.
                             <pre>{..., 'e</w>', 'th'}</pre>
                        </li>
                    </ol>
                    <p><strong>Result:</strong> After two iterations, "e</w>" and "th" have been added to the vocabulary based on the hypothetical frequency of pairs. The sentence's tokenization has been updated accordingly, with these new units treated as single tokens where they appear.</p>
                </div>

                <p class="text">So, after many thousands of such iterations, the vocabulary will contain a mix of individual characters, common sub-words (like "-ing", "pre-"), and full common words ("love", "hello").</p>
                <p class="text">When a ChatGPT user types a word like "loveable", the tokenizer will look up its vocabulary. If "love" is token 87 and "able" is token 21, then "loveable" might be broken down into <code>[87, 21]</code> and this sequence of numbers is given as input to the GPT model.</p>
                <p class="text">There are many online websites where you can visualize tokenizers in action for different LLMs and see how they break down text.</p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/imagegen-skill-issue.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Okay, so it is a skill issue - GPT-4 Image Generation - Ayush's Blog</title>
        <meta name="description" content="My journey from initial disappointment with GPT-4 image generation to understanding the crucial role of skilled prompting." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="Okay, so it is a skill issue - GPT-4 Image Generation - Ayush's Blog" />
        <meta property="og:description" content="My journey from initial disappointment with GPT-4 image generation to understanding the crucial role of skilled prompting." />
        <meta property="og:image" content="https://goyalayus.github.io/pin2.jpeg" /> <!-- Assuming pin2.jpeg is at the site root for OG -->
        <meta property="og:url" content="https://goyalayus.github.io/blog/imagegen-skill-issue.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <!-- Twitter Card data -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="Okay, so it is a skill issue - GPT-4 Image Generation - Ayush's Blog" />
        <meta name="twitter:description" content="My journey from initial disappointment with GPT-4 image generation to understanding the crucial role of skilled prompting." />
        <meta name="twitter:image" content="https://goyalayus.github.io/pin2.jpeg" /> <!-- Assuming pin2.jpeg is at the site root for Twitter -->
        <meta name="twitter:site" content="@goyalayus" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            .prompt-block {
                background-color: #f0f0f0;
                padding: 1rem;
                margin-bottom: 1.5rem;
                overflow-x: auto;
                white-space: pre-wrap;
                word-wrap: break-word;
                border: 1px solid #ddd;
                border-left: 4px solid var(--accent);
                font-size: 0.9em; /* Slightly smaller for prompt text */
            }
            .blog-image {
                display: block; /* Ensures margin auto works for centering */
                max-width: 250px; /* Restrict maximum width - MADE SMALLER */
                width: 100%; /* Responsive, but won't exceed max-width */
                height: auto; /* Maintain aspect ratio */
                margin: 1.5rem auto; /* Center the image if it's smaller than container */
                border: 1px solid #ccc;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            }
        </style>
    </head>
    <body>
        <header>
            <div class="profile">
                <a href="../index.html">
                    <img
                        src="../ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="../index.html" class="nav-link">ABOUT</a>
                <a href="../blog.html" class="nav-link active">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h1 class="section-title" style="font-size: 1.5rem; text-transform: none; border-bottom-width: 2px; margin-bottom: 1.5rem;">Okay, so it is a skill issue</h1>
                
                <p class="text">When I first heard the hype around gpt-4 imagegen on twitter, I though design was solved. like the examples were so good. but then when I tried it out it sucked. It was basically of no use other than making ghibhli images.</p>

                <p class="text">Yesterday I tried something, I picked this image below from pinterest.</p>
                
                <img src="../pin1.jpeg" alt="Original inspiration image from Pinterest for papercut texture pack design" class="blog-image">

                <p class="text">And gave it to gpt to analyze it. and then asked him to come up with a prompt to make this exact image. he gave me this prompt:</p>

                <pre class="prompt-block">
"Generate a promotional graphic for a digital 'Papercut Texture Pack.' The overall style is grunge, distressed, and vintage, evoking aged paper or a worn zine cover.

Background Image:
The background should feature a close-up, slightly desaturated image of a woman's face, primarily showing her right eye, nose, lips, and part of her ear and hair. Her gaze is soft and directed slightly off-camera. The image of the woman should be heavily processed with a prominent halftone dot pattern (CMYK-style dots if possible, with a reddish-brown dominant hue) and overlaid with various textures.

Color Palette:
The dominant colors for the background should be warm, earthy tones: deep reds, browns, and desaturated oranges. All text elements must be bright white for maximum contrast.

Textures and Overlays:
The entire image must be overlaid with heavy grunge textures, including:

    Realistic paper folds and creases (some sharp, some soft).

    Visible scratches, scuffs, and dust particles.

    A subtle, faded grid pattern (thin lines, possibly reddish or white) visible across the image.

    A general film grain or noise effect.

Typography and Text Elements (all in white, sans-serif font):

    Main Title (Center):

        Text: "PAPERCUT"

        Font: Very bold, uppercase, sans-serif (industrial or stencil-like feel, but solid letters).

        Effect: Letters should have a subtle internal texture or a slight "cutout" appearance, enhancing the papercut theme.

        Placement: Prominently centered horizontally, approximately mid-upper third of the image.

    Subtitle (Center, below Main Title):

        Text: "< TEXTURE PACK >" (including the angle brackets).

        Font: Sans-serif, uppercase, smaller than the main title but still bold.

        Placement: Directly beneath "PAPERCUT."

    Informational Text (Top Left):

        Text: "200 FILES"

        Font: Small, sans-serif, uppercase.

        Placement: Upper left quadrant.

    Informational Text (Top Right/Center-Right):

        Line 1: "100 FILES TRANSPARENT PNG" (with "PNG" aligned to the right of "TRANSPARENT")

        Line 2: "100 FILES MATTE / ORIGINAL PNG" (with "PNG" aligned to the right of "ORIGINAL")

        Font: Small, sans-serif, uppercase.

        Placement: Upper right quadrant, below the woman's eye level.

    Badges (Left, below Main Title level):

        Two circular badges.

        Left Badge Text: "4K" (white text on a dark, almost black circular background).

        Right Badge Text: "PNG" (white text on a dark, almost black circular background).

        Font: Sans-serif, uppercase.

        Placement: To the left of the vertical center, slightly below the "PAPERCUT" baseline.

    Informational Text (Right, below Badges level):

        Text: "FREE SAMPLE PACK AVAILABLE" (stacked in three lines: "FREE", "SAMPLE PACK", "AVAILABLE").

        Font: Small, sans-serif, uppercase.

        Placement: Right of the vertical center, roughly aligned horizontally with or slightly below the badges. Include a small, stylized downward-right pointing arrow icon (a simple 'L' shape rotated) above this text block.

    Website URL (Bottom Center):

        Text: "WWW.DESIGN-SYNDROME.CO"

        Font: Sans-serif, uppercase, medium size (larger than other informational text but smaller than subtitle).

        Placement: Bottom of the image, centered.

Composition and Lighting:

    The composition should feel layered, with text appearing on top of the textured image.

    Lighting should be diffuse, enhancing the textures, without harsh shadows from the text itself.

    Ensure text legibility against the complex background, perhaps with a very subtle drop shadow or outline on the text if needed, but prioritize a flat, integrated look if possible.

Overall Impression:
The final image must look like a physical, aged print that has been scanned, complete with imperfections. It should be atmospheric, tactile, and instantly communicate the product's aesthetic."
                </pre>

                <p class="text">And the output gpt gave is this image below.</p>

                <img src="../pin2.jpeg" alt="GPT-4 generated image based on detailed prompt for papercut texture pack design" class="blog-image">

                <p class="text">So I guess it's a skill issue afterall, if you know design gpt-4 imagegen is a great tool. if not you will not be able to produce great designs.</p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/startup-context.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Sharing all the context I've gathered - Ayush's Blog</title>
        <meta name="description" content="Ayush's blog post on startup and technical context for aspiring builders." />
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            main section ol.list > li { 
                margin-bottom: 1.5rem; 
            }
            main section ol.list > li:last-child {
                margin-bottom: 0.5rem; 
            }

            main section .list ol, 
            main section .list ul { 
                margin-top: 0.5rem;
                margin-left: 1.5rem;    
                list-style: initial;    
            }
            main section .list ol > li, 
            main section .list ul > li { 
                border-left: none;      
                padding-left: 0;        
                margin-bottom: 0.3rem;  
                list-style-position: outside; 
            }
            main section .list ol ol { list-style-type: decimal; }
            main section .list ol ul,
            main section .list ul ul { list-style-type: disc; } 

            .hover-container {
                position: relative;
                display: inline-block;
            }
            .hover-trigger {
                text-decoration: underline;
                text-decoration-style: dotted;
                cursor: help;
            }
            .hover-box { 
                position: absolute;
                display: none;
                width: 280px !important; 
                height: auto !important; 
                background-color: var(--bg-color);
                border: 1px solid var(--text-color);
                padding: 10px;
                font-size: 0.85rem;
                line-height: 1.4;
                text-align: left;
                z-index: 100;
                box-shadow: 0 0 10px rgba(0,0,0,0.3);
                border-radius: 5px;
                bottom: 30px; 
                left: 50%;
                transform: translateX(-50%);
                overflow: visible;
            }
            .hover-container:hover .hover-box {
                display: block;
            }
        </style>
    </head>

    <body>
        <header>
            <div class="profile">
                <a href="../index.html">
                    <img
                        src="../ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="../index.html" class="nav-link">ABOUT</a>
                <a href="../blog.html" class="nav-link active">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h1 class="section-title" style="font-size: 1.5rem; text-transform: none; border-bottom-width: 2px; margin-bottom: 1.5rem;">Sharing all the context I've gathered</h1>
                
                <p class="text">If you're looking to learn how to build a startup, start here. Once you go through these resources, there’s no fixed roadmap—only exploration.</p>
                <p class="text">Will be discussing two things:</p>
                <ul class="list"> 
                    <li>Startup context</li>
                    <li>Technical context</li>
                </ul>

                <h2 class="section-title">Startup Context</h2>
                <p class="text">The hard thing to build in this is not the "gyan", "lessons". The hard thing is to build "passion", "inspiration". And honestly I do not know how to make someone wanna do startups, to ignite the fire is really hard. But there are some traits:</p>
                <ul class="list"> 
                    <li>If you can't stop thinking about ideas</li>
                    <li>If you really wanna make a lot of money, become 
                        <span class="hover-container">
                            <span class="hover-trigger">financially free (i)</span>
                            <span class="hover-box">
                                You have enough money in bank that your requirements (4-5 lakhs a month for me) get sorted out by interest on that money.
                            </span>
                        </span>.
                    </li>
                    <li>Extension to above point, you are extremely dissatisfied by your current life. Even if you are earning lets say 5 lakhs a month.</li>
                    <li>You hate doing things which are done "just bcoz the norm", "esa hi hota hai". For example: going to college lectures.</li>
                </ul>
                <p class="text">So these are some traits, how can you <strong>build</strong> passion/these-traits?</p>
                <p class="text">Watching good content and being around people like that helps.</p>
                <p class="text">I highly recommend reading <a href="https://www.twodscapital.in/files/Rich-Dad-Poor-Dad-eBook.pdf" class="inline-link" target="_blank">Lesson 1 of Rich Dad Poor Dad</a>, <a href="https://www.readdiary.com/wp-content/uploads/2022/05/Steve-Jobs-PDFDrive-1.pdf" class="inline-link" target="_blank">Steve Jobs by Walter Isaacson</a> (recommended reading side by side the coursework not all at once).</p>
                <p class="text">And watching <a href="https://youtu.be/lXebD6nHD64?si=s0JPWuA6cjoWiqNe" class="inline-link" target="_blank">this video of Travis (founder of Uber) at IIT Bombay</a> and old <a href="https://www.youtube.com/watch?v=kdgaiyqvVA8&t=2086s&ab_channel=StoriesWithRustyPodcast" class="inline-link" target="_blank">podcasts of Varun Mayya</a>.</p>
                
                <p class="text">So now when we are done with lighting the fire lets go ahead with textbook lessons on startups:</p>
                <ol class="list"> 
                    <li>
                        <a href="https://www.youtube.com/playlist?list=PL5q_lef6zVkaTY_cT1k7qFNF2TidHCe-1" class="inline-link" target="_blank">YC How to Start a Startup</a> (till lecture number 14) --> watch them, rewatch them. I have watched each lecture at least 4 times by now.
                    </li>
                    <li>Paul Graham top 10 essays:
                        <ol> 
                            <li><a href="http://paulgraham.com/start.html" class="inline-link" target="_blank">How to Start a Startup</a></li>
                            <li><a href="http://paulgraham.com/ds.html" class="inline-link" target="_blank">Do Things that Don't Scale</a></li>
                            <li><a href="http://paulgraham.com/growth.html" class="inline-link" target="_blank">Startup = Growth</a></li>
                            <li><a href="http://paulgraham.com/makersschedule.html" class="inline-link" target="_blank">Maker's Schedule, Manager's Schedule</a></li>
                            <li><a href="http://paulgraham.com/ideas.html" class="inline-link" target="_blank">Ideas for Startups</a></li>
                            <li><a href="http://paulgraham.com/startupmistakes.html" class="inline-link" target="_blank">The 18 Mistakes That Kill Startups</a></li>
                            <li><a href="http://paulgraham.com/startupideas.html" class="inline-link" target="_blank">How to Get Startup Ideas</a></li>
                            <li><a href="http://paulgraham.com/good.html" class="inline-link" target="_blank">Be Good</a></li>
                            <li><a href="http://paulgraham.com/really.html" class="inline-link" target="_blank">What Startups Are Really Like</a></li>
                            <li><a href="http://paulgraham.com/hackinvest.html" class="inline-link" target="_blank">The Hacker’s Guide to Investors</a></li>
                        </ol>
                    </li>
                    <li>Read <a href="https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf" class="inline-link" target="_blank">The Almanack of Naval Ravikant</a> and <a href="https://morfene.com/021.pdf" class="inline-link" target="_blank">Zero to One</a>.</li>
                    <li>Sajith Pai:
                        <ul> 
                            <li>Talk: <a href="https://www.youtube.com/watch?v=WI9pTXw59DY&t=3630s&pp=ygURc2FqaXRoIHBhaSBhbnRsZXI%3D" class="inline-link" target="_blank">here</a></li>
                            <li>Articles:
                                <ol>
                                    <li><a href="https://sajithpai.com/the-indus-valley-playbook/" class="inline-link" target="_blank">The Indus Valley Playbook</a></li>
                                    <li><a href="https://sajithpai.com/the-busy-founders-guide-to-pmf/" class="inline-link" target="_blank">The Busy Founder's Guide to PMF</a></li>
                                    <li><a href="https://sajithpai.com/tam-notes-thoughts/" class="inline-link" target="_blank">TAM</a></li>
                                </ol>
                            </li>
                        </ul>
                    </li>
                    <li>Kunal Shah Delta 4 framework: <a href="https://www.youtube.com/watch?v=sk6hA6JR9gI&pp=ygUZa3VuYWwgc2hhaCAgaWltIGFobWVkYWJhZA%3D%3D" class="inline-link" target="_blank">watch here</a>.</li>
                </ol>

                <h2 class="section-title">Technical Context</h2>
                <ul class="list"> 
                    <li>HTML: <a href="https://www.youtube.com/watch?v=BsDoLVMnmZs&pp=ygUSY29kZXdpdGhoYXJyeSBodG1s" class="inline-link" target="_blank">CodeWithHarry HTML</a></li>
                    <li>CSS: <a href="https://www.youtube.com/watch?v=Edsxf_NBFrw&ab_channel=CodeWithHarry" class="inline-link" target="_blank">CodeWithHarry CSS</a> (only study till media queries)</li>
                </ul>
                <p class="text">If you have never coded before, watch this too: <a href="https://www.youtube.com/playlist?list=PLhQjrBD2T381WAHyx1pq-sBfykqMBI7V4" class="inline-link" target="_blank">CS50x lectures</a>.</p>
                <p class="text">Now after completing all these, we will now move to React and Node.js. Will be studying them through the course of Harkirat Singh (<em>some link I'll add later</em>).</p>

                <h3 class="section-title" style="font-size: 1.1rem; border-bottom-width: 1px;">AI ML</h3>
                <ul class="list"> 
                    <li>Linear Regression</li>
                    <li>Logistic Regression</li>
                    <li>Softmax function</li>
                    <li>Neural Networks:
                        <ul> 
                            <li>Lecture 1: <a href="https://www.youtube.com/watch?v=MfIjxPh6Pys&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=11&pp=iAQB" class="inline-link" target="_blank">Link</a></li>
                            <li>Lecture 2: <a href="https://www.youtube.com/watch?v=zUazLXZZA2U&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=12&pp=iAQB" class="inline-link" target="_blank">Link</a></li>
                            <li>Lecture 3 (Karpathy): <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" class="inline-link" target="_blank">Link</a></li>
                        </ul>
                    </li>
                    <li>Transformers:
                        <ul> 
                            <li>Lecture 1 (Playlist): <a href="https://www.youtube.com/playlist?list=PLu2S64ZsXxCNOW_jPjENkQZqC6DhMzZOL" class="inline-link" target="_blank">Link</a></li>
                            <li>Lecture 2 (Karpathy): <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7&t=1s&pp=iAQB" class="inline-link" target="_blank">Link</a></li>
                        </ul>
                    </li>
                </ul>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/students-who-stammer.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Students in Tech who stutter/stammer - Ayush's Blog</title>
        <meta name="description" content="An exclusive community for tech students who stutter/stammer. Learn why it's different, who's behind it, and how to join." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="Students in Tech who stutter/stammer - Ayush's Blog" />
        <meta property="og:description" content="An exclusive community for tech students who stutter/stammer. Learn why it's different, who's behind it, and how to join." />
        <meta property="og:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta property="og:url" content="https://goyalayus.github.io/blog/students-who-stammer.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <!-- Twitter Card data -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="Students in Tech who stutter/stammer - Ayush's Blog" />
        <meta name="twitter:description" content="An exclusive community for tech students who stutter/stammer. Learn why it's different, who's behind it, and how to join." />
        <meta name="twitter:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta name="twitter:site" content="@goyalayus" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <!-- Link to the main stylesheet, some styles will be overridden below -->
        <link rel="stylesheet" href="../style.css" />
        <style>
            body {
                background-color: #ffffff; /* Plain white background */
                background-image: none; /* Remove grid pattern from style.css */
                font-family: 'Space Mono', monospace; /* Or a sans-serif like 'Arial', sans-serif for closer match to image */
                color: #333333; /* Standard dark gray text color */
                line-height: 1.6;
                padding: 0.5rem 1rem 2rem 1rem; /* Reduced top padding, kept original L/R/B */
                max-width: 700px; /* Content readability width */
                margin: 0.5rem auto 2rem auto; /* Reduced top margin, kept original bottom/auto L/R */
            }

            main {
                padding-top: 0; /* Remove padding at the top of main content area */
            }

            .page-main-title {
                font-size: 2rem; /* Large title like "Arpit's Site" */
                font-weight: 700;
                color: #191919;
                margin-top: 0; /* Remove top margin for H1 */
                margin-bottom: 1.5rem; /* Adjusted bottom margin for balance */
                border-bottom: none; /* Remove default border from section-title */
                text-transform: none; /* Keep case as is */
                text-align: left;
            }

            .content-section h2 { /* For subheadings like "who am I?" */
                font-size: 1.4rem; /* Like "About", "Posts" in image */
                font-weight: 700;
                color: #191919;
                margin-top: 2.5rem;
                margin-bottom: 1rem;
                border-bottom: none;
                text-transform: none;
            }

            .content-section p {
                font-size: 1rem;
                margin-bottom: 1rem;
                color: #333333;
            }

            .content-section ul {
                list-style: disc; /* Standard bullet points */
                margin-left: 20px; /* Indent list */
                padding-left: 5px; /* Space for bullets */
                margin-bottom: 1.5rem;
            }

            .content-section ul li {
                margin-bottom: 0.75rem;
                border-left: none; /* Remove side border from main style.css list style */
                padding-left: 0;
            }

            .content-section a,
            .content-section a.inline-link { /* Style all links within content consistently */
                color: #007bff; /* Blue for links, like in the image */
                text-decoration: underline; /* Underline links as in image's paragraph links */
            }
            .content-section a:hover,
            .content-section a.inline-link:hover {
                color: #0056b3; /* Darker blue on hover */
            }

            /* Override any general section title styling from style.css if not desired here */
            main section .section-title { 
                /* This class is used for the main title now, handled by .page-main-title */
            }

        </style>
    </head>
    <body>
        <!-- Header removed as per request -->

        <main>
            <section class="content-section">
                <h1 class="page-main-title">Students in Tech who stutter/stammer</h1>
                
                <h2>so there a lot of communities in the world what's different?</h2>
                <ul>
                    <li>
                        i realized that in all these communities there are people from all ages, professions and backgrounds. and it's really tough to hold meaningful conversations. i'm way more interested in chatting with a 20 y/o girl who's into robotics or app design from china than some 40 y/o uncle working on a farm.
                    </li>
                    <li>
                        so i'm building this exclusive community for tech students who stammer. i do a basic intro call where i assess if the person will vibe with the community and actually stutters. to keep the exclusivity, tight-knit feel and all that.
                    </li>
                </ul>

                <p>that's it.</p>

                <h2>who am I?</h2>
                <p>
                    I am a pre-final year student from india who is interested in software and startups. you can check more about me <a href="https://goyalayus.github.io/">here</a>.
                </p>
                
                <h2>how to join?</h2>
                <p>
                    just text me on whatsapp at +91 7976400427
                </p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/summer-goals.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>What am I doing this summer - Ayush's Blog</title>
        <meta name="description" content="Ayush's summer goals, focusing on AI/ML mastery and personal development." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="What am I doing this summer - Ayush's Blog" />
        <meta property="og:description" content="My summer goals: diving deep into AI/ML to become a versatile founder and dedicating time to improve my speech." />
        <meta property="og:image" content="https://goyalayus.github.io/summer-goals-preview.png" />  <!-- <= CORRECTED EXTENSION -->
        <meta property="og:url" content="https://goyalayus.github.io/blog/summer-goals.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <!-- Twitter Card data (optional, but good practice) -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="What am I doing this summer - Ayush's Blog" />
        <meta name="twitter:description" content="My summer goals: diving deep into AI/ML to become a versatile founder and dedicating time to improve my speech." />
        <meta name="twitter:image" content="https://goyalayus.github.io/summer-goals-preview.png" /> <!-- <= CORRECTED EXTENSION -->
        <meta name="twitter:site" content="@goyalayus">

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            main section ol.list > li { 
                margin-bottom: 1.5rem; 
            }
            main section ol.list > li:last-child {
                margin-bottom: 0.5rem; 
            }

            main section .list ol, 
            main section .list ul { 
                margin-top: 0.5rem;
                margin-left: 1.5rem;    
                list-style: initial;    
            }
            main section .list ol > li, 
            main section .list ul > li { 
                border-left: none;      
                padding-left: 0;        
                margin-bottom: 0.3rem;  
                list-style-position: outside; 
            }
            main section .list ol ol { list-style-type: decimal; }
            main section .list ol ul,
            main section .list ul ul { list-style-type: disc; } 

            .hover-container {
                position: relative;
                display: inline-block;
            }
            .hover-trigger {
                text-decoration: underline;
                text-decoration-style: dotted;
                cursor: help;
            }
            .hover-box { 
                position: absolute;
                display: none;
                width: 280px !important; 
                height: auto !important; 
                background-color: var(--bg-color);
                border: 1px solid var(--text-color);
                padding: 10px;
                font-size: 0.85rem;
                line-height: 1.4;
                text-align: left;
                z-index: 100;
                box-shadow: 0 0 10px rgba(0,0,0,0.3);
                border-radius: 5px;
                bottom: 30px; 
                left: 50%;
                transform: translateX(-50%);
                overflow: visible;
            }
            .hover-container:hover .hover-box {
                display: block;
            }
        </style>
    </head>
    <body>
        <header>
            <div class="profile">
                <a href="../index.html">
                    <img
                        src="../ayush.jpeg"
                        alt="ayush's photo"
                        class="profile-img"
                    />
                    <h1 class="site-title">ayush</h1>
                </a>
            </div>
            <nav class="nav-links">
                <a href="../index.html" class="nav-link">ABOUT</a>
                <a href="../blog.html" class="nav-link active">BLOG</a>
            </nav>
        </header>

        <main>
            <section>
                <h1 class="section-title" style="font-size: 1.5rem; text-transform: none; border-bottom-width: 2px; margin-bottom: 1.5rem;">What am I doing this summer.</h1>
                
                <p class="text">      
Posting your goals to world keeps you super accountable

    </p>

                <h2 class="section-title" style="text-transform: none;">1. I want to become the founder who can start any AI Company.</h2>
                <ul class="list">
                    <li>Learn how <a href="https://github.com/karpathy/nanoGPT" class="inline-link" target="_blank">gpt-2</a> works</li>
                    <li><a href="https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=2262s&pp=iAQB" class="inline-link" target="_blank">View</a> some optimizing techniques</li>
                    <li>Studying DeepSeek V3 code</li>
                    <li>Multimodal LLMs</li>
                    <li>Reinforcement learning basics</li>
                    <li>Studying DeepSeek R1 code</li>
                    <li>What the fuck is MCP, what's wrong with APIs?</li>
                    <li>What is this "distillation"?</li>
                    <li>How do we give "1 million context window"?</li>
                    <li>Evals</li>
                    <li>How do Audio agents work?</li>
                    <li>Agentic browsers</li>
                    <li>How good RAG really is, especially over code</li>
                    <li>Diffusion models</li>
                    <li>GPT-4 Image generation</li>
                    <li>How does deep research work?</li>
                </ul>

                <h2 class="section-title" style="text-transform: none;">2. Work on my stammering atleast 1/hr a day.</h2>
                <ul class="list">
                    <li>Reading</li>
                    <li>Sequences</li>
                    <li>Camera story telling</li>
                    <li>Google Meets / phone calls</li>
                </ul>

                <p class="text" style="margin-top: 2rem; font-style: italic;">
                    P.S. - I will be blogging about a lot of these topics, just for proof of work. (I hate it though, just knowing I am Ayush Goyal should be enough)
                </p>
            </section>
        </main>
    </body>
</html>



================================================
File: blog/two-startups.html
================================================
<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>2 Startups - Ayush's Blog</title>
        <meta name="description" content="The critical distinction between pre-PMF and post-PMF phases in startups. Why understanding this two-part nature is key for founders." />

        <!-- Open Graph / Facebook / WhatsApp -->
        <meta property="og:type" content="article" />
        <meta property="og:title" content="2 Startups - Ayush's Blog" />
        <meta property="og:description" content="The critical distinction between pre-PMF and post-PMF phases in startups. Why understanding this two-part nature is key for founders." />
        <meta property="og:image" content="https://goyalayus.github.io/ayush.jpeg" />
        <meta property="og:url" content="https://goyalayus.github.io/blog/two-startups.html" />
        <meta property="og:site_name" content="Ayush's Blog" />

        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&display=swap"
            rel="stylesheet"
        />
        <link rel="stylesheet" href="../style.css" />
        <style>
            body {
                background-color: #ffffff;
                background-image: none;
                font-family: 'Space Mono', monospace;
                color: #333333;
                line-height: 1.6;
                padding: 0.5rem 1rem 2rem 1rem;
                max-width: 700px;
                margin: 0.5rem auto 2rem auto;
            }

            main {
                padding-top: 0;
            }

            .page-main-title {
                font-size: 2rem;
                font-weight: 700;
                color: #191919;
                margin-top: 0;
                margin-bottom: 1.5rem;
                border-bottom: none;
                text-transform: none;
                text-align: left;
            }

            .content-section h2 { /* Default h2 style */
                font-size: 1.4rem;
                font-weight: 700;
                color: #191919;
                margin-top: 2.5rem;
                margin-bottom: 1rem;
                border-bottom: none;
                text-transform: none;
            }

            .subheading-plain { /* For "What's PMF?" when not in a box, or as a base */
                font-size: 1.4rem;
                font-weight: 400;
                color: #191919;
                margin-top: 2.5rem;
                margin-bottom: 1rem;
                border-bottom: none;
                text-transform: none;
            }

            .content-section p {
                font-size: 1rem;
                margin-bottom: 1rem;
                color: #333333;
            }

            .content-section ul {
                list-style: disc;
                margin-left: 20px;
                padding-left: 5px;
                margin-bottom: 1.5rem;
            }

            .content-section ul li {
                margin-bottom: 0.75rem;
                border-left: none;
                padding-left: 0;
            }
             .content-section ul li strong {
                font-weight: 700;
            }

            .pmf-phases {
                list-style-type: decimal;
                padding-left: 25px;
                margin-top: 0.5rem;
                margin-bottom: 1.5rem;
                color: var(--accent);
            }

            .pmf-phases li {
                font-weight: bold;
                margin-bottom: 0.5rem;
                border-left: none;
                padding-left: 0;
            }
            
            .orange-highlight {
                color: var(--accent);
                margin-top: 1.5rem;
                margin-bottom: 1.5rem;
            }

            /* New style for the definition box */
            .definition-box {
                background-color: #f9f9f9; /* Slightly off-white background */
                border: 1px solid #e0e0e0; /* Softer border color */
                border-left: 4px solid var(--accent); /* Accent on the left */
                padding: 1.25rem; /* Internal spacing */
                margin-top: 2rem; /* Space above the box */
                margin-bottom: 2rem; /* Space below the box */
                border-radius: 4px; /* Slightly rounded corners */
            }

            .definition-box .subheading-plain { /* "What's PMF?" heading inside the box */
                margin-top: 0; /* Remove top margin as box provides it */
                margin-bottom: 0.75rem; /* Space below heading inside box */
                font-size: 1.25rem; /* Slightly smaller heading for inside a box */
                font-weight: 700; /* Make it bold again for emphasis as a title within the box */
                color: #191919; /* Ensure it's dark text */
            }

            .definition-box p {
                font-size: 0.95rem; /* Slightly smaller text for definition */
                color: #444444; /* Slightly lighter text color for definition */
                line-height: 1.5;
            }
            .definition-box p:last-child {
                margin-bottom: 0; /* Remove bottom margin for last paragraph in box */
            }
            /* End of definition box style */


            .content-section a,
            .content-section a.inline-link {
                color: #007bff;
                text-decoration: underline;
            }

            .content-section a:hover,
            .content-section a.inline-link:hover {
                color: #0056b3;
            }

            .back-arrow {
                display: inline-block;
                margin-bottom: 1rem;
                font-size: 1rem;
                color: #007bff;
                text-decoration: none;
                font-weight: bold;
                transition: color 0.2s ease;
            }

            .back-arrow:hover {
                color: #0056b3;
                text-decoration: underline;
            }
        </style>
    </head>
    <body>
        <main>
            <a href="https://goyalayus.github.io" class="back-arrow" aria-label="Go to homepage">⬅ Home</a>

            <section class="content-section">
                <h1 class="page-main-title">2 Startups</h1>

                <p>Startups aren't a single thing; they're a <strong>two-parter:</strong></p>
                <ol class="pmf-phases">
                    <li>Before Product-Market Fit (PMF)</li>
                    <li>After Product-Market Fit (PMF)</li>
                </ol>

                <div class="definition-box">
                    <h2 class="subheading-plain">What's PMF?</h2>
                    <p>It's when you <strong>know</strong> people need your thing, a small crew <strong>loves</strong> loves it, retention's high, and you've figured out scaling. You just need cash to scale.</p>
                </div>

                <p>The thing is, these two phases aren't just a bit different; they're totally different, a full 180. The rules and physics for both are completely different.</p>
                
                <p class="orange-highlight">Post-PMF startups need a totally different name. The damage from this lack of distinction is insane; calling both by the same name should be criminal.</p>

                <p>So many times you're watching a founder interview, they're giving post-PMF advice, and you try to apply it to pre-PMF. And obviously, the advice just doesn't work there.</p>

                <p>All Y Combinator advice is generally pre-PMF advice.</p>
                
                <p>Me and my friends almost never even discuss the post-PMF startup stuff. There's zero point talking or planning or prepping for that bit until you actually hit PMF. So our focus is super narrowed on finding pre-PMF advice and actually doing it.</p>
            </section>
        </main>
    </body>
</html>


